---
title: "Exercise: Spatial Upscaling"
author: "Applied Geodata Science 2 (HS 2023)"
date: "Tino Schneidewind"
output: 
  html_document:
    code_folding: show
---



```{r setup, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(raster)
library(leaflet)
library(sf)
library(hrbrthemes)

```

```{r library, message=FALSE, warning=FALSE,}
# Libraries

library(recipes)
library(dplyr)
library(ggplot2)
library(tidyterra)
library(ranger)
library(lattice)
library(caret)
library(rnaturalearth)
library(rnaturalearthdata)
library(viridis)
library(gridExtra) 
```



```{r data, message=FALSE}
# Load Data

df <- readr::read_csv("https://raw.githubusercontent.com/stineb/leafnp_data/main/data/leafnp_tian_et_al.csv")

common_species <- df |> # get species names
  group_by(Species) |> 
  summarise(count = n()) |> 
  arrange(desc(count)) |> 
  slice(1:50) |> 
  pull(Species)

dfs <- df |>   # select only needed variables
  dplyr::select(leafN, lon, lat, elv, mat, map, ndep, mai, Species) |> 
  filter(Species %in% common_species)

set.seed(06122023) # for reproducibility
```

<br>

#### **Ludwig et al.**
#### 1) Explain the difference between a random cross-validation and a spatial cross-validation.

There are differences in the variable selection in both methods.
Random cross validation is able to reproduce clustered data well,
but might not be suitable for spatial scaling due to a strong clustering of the reference data and issues with quality control for the spatial predictions. Consequently it is hard to quality check predictions at a new sample site.

The advantage of spatial cross-validation is, that it works better with clustered reference data because it focuses on choosing variables that improve the predictability of new areas. In random cross validation, we measure the distance between point and use this distance to estimate points in the space that is known to us. Spatial cross-validation serves as a solution for the delta between the sample-to-sample distribution and the sample-to-prediction distribution. Spatial cross validation performed worse than random cross validation, but had a larger area of applicability. 


#### 2) Do you see an alternative to measuring a distance that considers the task of spatial upscaling based on environmental covariates more directly?
We could use the distance between environmental covariates respective to their means, for example temperature, radiation and water cycle means. 

<br>

#### **Random Cross Validation**
#### Use Random Forest to perform a 5-fold cross-validation with the leaf nitrogen data. Report the mean RMSE and R2 across cross-validation folds.
```{r random_cv, warning=FALSE, message=FALSE}
# model formulation
mf <- recipes::recipe(leafN ~ elv + mat + map + ndep + mai + Species, 
                      data = dfs) |> 
  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())

# random forest model
mod <- train(
  mf, 
  data = dfs %>%
    drop_na(), 
  method = "ranger",
  trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
  tuneGrid = expand.grid( .mtry = 3,
                          .min.node.size = 12,
                          .splitrule = "variance"),
  metric = "RMSE",
  replace = FALSE,
  sample.fraction = 0.5,
  num.trees = 2000,           
  seed = 1982                
)

# model performance
print(mod$results[,4:5])
```

<br>

#### **Spatial Cross Validation**

#### 1) What do you observe on the gobal map of observation sites? Discuss the potential implications of the geographical distribution of data points for spatial upscaling.
We can observe severe clustering of sample sites in Europe and China. There are only very few samples in North and South America, and only 2 sample sites in Australia and 1 in Africa. This leads to poor representation of the global variety in the feature space. As a result, spatial cross validation could be advantageous compared to random cross validation concerning later spatial up scaling to unknown sites. 

#### 2) Perform a spatial cross-validation. Plot points on a global map, showing the five clusters with distinct colors.

To control the randomness of my analysis, I used set.seed(06122023). This ensures repeatable model results and consistency in my conclusions.
```{r k_means_spatial, message=FALSE, warning=FALSE}
# cluster the data by column lat and lon
dfs$clust <- as.factor(kmeans(
  dfs[,2:3],
  centers = 5
)$cluster)

```
```{r spatial_cv, echo=FALSE, message=FALSE, warning=FALSE}
coast <- rnaturalearth::ne_coastline(scale = 110, returnclass = "sf")

# colors for plot
my_colors = c("brown1", "gold", "green", "skyblue", "purple")

# Map of leafN ditribution with clusters
ggplot() +
  # plot coastline
  geom_sf(data = coast,
          colour = 'black',
          size = 0.2) +
  
  # set extent in longitude and latitude
  coord_sf(
    ylim = c(-60, 80),
    expand = FALSE) +  # to draw map strictly bounded by the specified extent
  
  # plot points on map
  geom_point(data = dfs, aes(x = lon, y = lat, color= clust), size = 0.6) +
  scale_color_manual(values=my_colors) +
  labs(x = "", y = "") +
  theme(legend.position = "bottom") +
  labs(title="Modelinput: lon+lat")

```

#### 3) Plot the distribution of leaf N by cluster.
```{r plot_leaf_distribution_spatial, echo=FALSE, message=FALSE, warning=FALSE}
plot1 <- dfs |>
  ggplot( aes(x=clust, y=leafN, fill=clust)) +
  geom_boxplot() +
  scale_fill_manual(values=my_colors) +
  theme_ipsum() +
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  ggtitle("leafN distribution") +
  xlab("leafN")



plot2 <- dfs |>
  ggplot(aes(x=leafN)) +
  geom_histogram(fill = "brown1", color = "black") +
  facet_wrap(clust ~ .) +
  theme_ipsum()

grid.arrange(plot1, plot2, ncol = 2) 
```

#### 4) Split your data into five folds that correspond to the geographical clusters identified by in (2.), and fit a random forest model performing a 5-fold cross-validation with the clusters as folds. Report the RMSE and the R2 determined on each of the five folds.
```{r randomforest_spatial, message=FALSE}
# training folds from kmeans clusters
group_folds_train <- purrr::map(
  seq(length(unique(dfs$clust))),
  ~ {
    dfs |> 
      select(clust) |> 
      mutate(idx = 1:n()) |> 
      filter(clust != .) |> 
      pull(idx)})

# testing folds from kmeans clusters
group_folds_test <- purrr::map(
  seq(length(unique(dfs$clust))),
  ~ {
    dfs |> 
      select(clust) |> 
      mutate(idx = 1:n()) |> 
      filter(clust == .) |> 
      pull(idx)})

# random forest function
train_test_by_fold <- function(df, idx_train, idx_val){ 
  mod <- ranger::ranger(
    x =  df[idx_train,2:9],  
    y =  df$leafN[idx_train]   
  )
  
  pred <- predict(mod, data = df[idx_val,2:9])
  
  rsq <-  summary(lm(pred$predictions~df$leafN[idx_val]))$r.squared   
              
  rmse <- sqrt( mean (( df$leafN[idx_val]-pred$predictions )^2 )) 
              
  return(tibble(rsq = rsq, rmse = rmse))
}

# apply function on predefined folds
out <- purrr::map2_dfr(
  group_folds_train,
  group_folds_test,
  ~train_test_by_fold(dfs,.x, .y) 
) |> 
  mutate(test_fold = 1:5)
out
```

#### 5) Compare the results of the spatial cross-validation to the results of the random cross-validation and discuss reasons for why you observe a difference in the cross-validation metrics.

Spatial cross-validation performed significantly worse than random cross-validation, with considerable differences between folds. Visually, these differences in RSME and R2 in spatial cross-validation correspond to variance in the fold size distribution. Fold 3 has the highest count and also the lowest RSME and highest R2. Fold 2 has by far the lowest count and also the highest RSME and lowest R2. The smaller the training data, the weaker the predictability performance. As observed in Ludwig et al. (2023), random cross-validation outperforms spatial cross-validation, however, we did not investigate the difference in area of applicability, where the advantages of spatial cross-validation lie. 

<br>

#### **Environmental Cross Validation**
#### 1) Perform a custom cross-validation as above, but this time considering five clusters of points in environmental space - spanned by the mean annual precipitation and the mean annual temperature. Report the R-squared and the RMSE on the validation set of each of the five folds.

```{r kmeans_environmental,  message=FALSE, warning=FALSE}
# cluster the data by column mat and map
dfs$clust_env <- as.factor(kmeans(
  dfs[,5:6],
  centers = 5
)$cluster)

```


```{r env_cvs, echo=FALSE, message=FALSE, warning=FALSE}
# plot world map cluster distribution on mat+map
ggplot() +
  # plot coastline
  geom_sf(data = coast,
          colour = 'black',
          size = 0.2) +
  
  # set extent in longitude and latitude
  coord_sf(
    ylim = c(-60, 80),
    expand = FALSE) +  # to draw map strictly bounded by the specified extent
  
  # plot points on map
  geom_point(data = dfs, aes(x = lon, y = lat, color= clust_env), size = 0.6) +
  scale_color_manual(values=my_colors) +
  labs(x = "", y = "") +
  theme(legend.position = "bottom") +
  labs(title="Modelinput: mat+map")


## Plot the distribution of leaf N by cluster.
plot3 <- dfs %>%
  ggplot( aes(x=clust_env, y=leafN, fill=clust_env)) +
  geom_boxplot() +
  scale_fill_manual(values=my_colors) +
  theme_ipsum() +
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  ggtitle("leafN distribution") +
  xlab("leafN")



plot4 <- dfs %>% 
  ggplot(aes(x=leafN)) +
  geom_histogram(fill = "brown1", color = "black") +
  facet_wrap(~clust_env) +
  theme_ipsum()


grid.arrange(plot3, plot4, ncol = 2) 
```
```{r env_cv,  message=FALSE}
# training folds from new kmeans clusters
group_folds_train <- purrr::map(
  seq(length(unique(dfs$clust))),
  ~ {
    dfs |> 
      select(clust_env) |> 
      mutate(idx = 1:n()) |> 
      filter(clust_env != .) |> 
      pull(idx)})

# testing folds from new kmeans clusters
group_folds_test <- purrr::map(
  seq(length(unique(dfs$clust))),
  ~ {
    dfs |> 
      select(clust_env) |> 
      mutate(idx = 1:n()) |> 
      filter(clust_env == .) |> 
      pull(idx)})

# apply function on predefined new folds
out <- purrr::map2_dfr(
  group_folds_train,
  group_folds_test,
  ~train_test_by_fold(dfs,.x, .y) 
) |> 
  mutate(test_fold = 1:5)
out
```

Environmental spatial-cross validation performed considerably better than regular spacial cross-validation but worse than random cross validation. Also the performance variance between folds in environmental cross-validation was very low compared to spatial cross-validation, even though the fold size was highly variable in environmental cross-validation. In conclusion, environmental variables are more suitable for clustering than spatial variables.

<br>











